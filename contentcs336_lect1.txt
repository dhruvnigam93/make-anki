Source : course cs336
url : https://www.youtube.com/watch?v=SQ3fZ1sAqXI&list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_
content :
Welcome, everyone.
This is CS 336-- Language Models from Scratch.
And this is the core staff. So I'm Percy, one of your instructors.
I'm really excited about this class, because it really allows you to see the whole language modeling building pipeline end to end,
including data systems and modeling. Tatsu, I'll be co-teaching with him, so I'll let everyone introduce themselves.
Hi, everyone. I'm Tatsu. I'm one of the co-instructors. I'll be giving lecture in a week or two, probably.
A few weeks. I'm really excited about this class. Percy and I spent a while being a little disgruntled, thinking,
what's the really deep technical stuff that we can teach our students today? And I think one of the things that has really-- you
got to build it from scratch to understand it, so I'm hoping that that's of the ethos that you'll take away from.
Hey, everyone. I'm Rohith I actually failed this class when I took it.
[LAUGHTER] But now I'm your TA. So you know what did they say, anything is possible.
[LAUGHTER] Hey, everyone. I'm Neil.
I'm a third year student, PhD student in the CS Department. I work with Tatsu and [INAUDIBLE].
Yeah, mostly interested in research on synthetic data and language models. Reasoning, all that stuff.
So yeah, should be a fun quarter. Hey, guys. I'm Marcel. I'm a second-year PhD.
I work mostly with [INAUDIBLE]. These days, I work [INAUDIBLE].
And he was a topper of many leaderboards from last year, so he's the number to beat.
OK. All right. Well, thanks, everyone. So let's continue. As Tatsu mentioned, this is the second time
we're teaching the class. We've grown the class by around 50%. I have three TAs instead of two.
And one big thing is we're making all the lectures on YouTube so that the world can learn how to build
language models from scratch. OK. So why do we decide to make this course and endure all the pain?
So let's ask GPT-4. So if you ask it, why teach a course on building language
models from scratch, the reply is, teaching a course
provides foundational understanding of techniques, fosters innovation.
The typical generic blathers. OK, so here's the real reason. So we're in a bit of a crisis, I would say.
Researchers are becoming more and more disconnected from the underlying technology.
Eight years ago, researchers would implement and train their own models in AI.
Even six years ago, you would at least take the models like BERT and download them and fine-tune them.
And now, many people can just get away with prompting a proprietary model.
So this is not necessarily bad, right, because as you enter these layers of abstraction,
we can all do more. And a lot of research has been unlocked by the simplicity of being able to prompt a language model.
And I do a fair share of prompting. So there's nothing wrong with that.
But let's also remember that these abstractions are leaky. So in contrast to programming languages on operating systems,
you don't really understand what the abstraction is. It's a string in and string out, I guess.
And I would say that there's still a lot of fundamental research to be done that requires tearing up
the stack and co-designing different aspects of the data and the systems and the model. And I think really that full understanding of this technology
is necessary for fundamental research. So that's why this class exists.
We want to enable the fundamental research to continue. And our philosophy is to understand it,
you have to build it. So there's one small problem here. And this is because of the industrialization
of language models. So GPT-4 has rumored to be 1.8 trillion parameters,
cost $100 million to train. You have xAI building the clusters with 200,000 H100s,
if you can imagine that. There's an investment of over $500 billion,
supposedly, over four years. So these are pretty large numbers, right.
And furthermore, there's no public details on how these models are being built. Here from GPT-4--
this is even two years ago-- they very honestly say that due to the competitive landscape
and safety limitations, we're going to disclose no details. OK, so this is the state of the world right now.
And so in some sense, frontier models are out of reach for us. So if you came into this class thinking
you're each going to train your own GPT-4, sorry.
So we're going to build small language models. But the problem is that these might not be representative.
And here are some of two examples to illustrate why. So here's a simple one.
If you look at the fraction of FLOPs spent in the attention layers of a transformer versus an MLP,
this changes quite a bit. So this is a tweet from Stephen Roller from quite a few years ago.
But this is still true. If you look at small models, it looks
like the number of FLOPs in the attention versus the MLP layers are roughly comparable.
But if you go up to 175 billion, then the MLPs really dominate.
Right, so why does this matter? Well, if you spend a lot of time at small scale
and you're optimizing the tension, you might be optimizing the wrong thing, because at larger scale, it gets washed out.
This is a simple example, because you can literally make this plot without actually any compute.
It's napkin math. Here's something that's a little bit harder to grapple with,
is this emergent behavior. So this is a paper from Jason Wei from 2022.
And here, this plot shows that as you increase
the amount of training FLOPs and you look at accuracy on a bunch of tasks,
you'll see that for a while, it looks like the accuracy, nothing is happening. And all of a sudden, you get these emergent
of various phenomena, like in-context learning. So if you were hanging around at this scale,
you would be concluding that well, these language models really don't work. When, in fact, you had to scale up to get that behavior.
So don't despair. We can still learn something in this class.
But we have to be very precise about what we're learning. So there's three types of knowledge.
There's the mechanics of how things work. This, we can teach you. We can teach you what a transformer is.
You can implement a transformer. We can teach you how model parallelism leverages GPUs efficiently.
These are just the raw ingredients, the mechanics. So that's fine.
We can also teach you mindset. So this is something a bit more subtle and seems a little bit fuzzy.
But this is actually, in some ways, more important, I would say.
Because the mindset that we're going to take is that we want to squeeze as most
out of the hardware as possible and take scaling seriously. Right, because in some sense, the mechanics, all of those
will see later that all of these ingredients have been around for a while. But it was really, I think, the scaling mindset
that OpenAI pioneered that led to this next generation of AI
models. So mindset, I think, hopefully we can bang into you that to think in a certain way.
And then thirdly is intuitions. And this is about which data and modeling
decisions lead to good models. This, unfortunately, we can only partially teach you.
And this is because what architectures and what data sets work at low scales
might not be the same ones that work at large scales.
But hopefully you got 2 and 1/2 out of three. So that's pretty good bang for your buck.
OK, speaking of intuitions, there's a sort of, I guess, sad reality of things that you can tell a lot of stories
about why certain things in the transformer are the way they are. But sometimes it's just you do the experiments
and the experiments speak. So, for example, there's this known Shazeer paper that introduced the SwiGLU, which is something
that we'll see a bit more in this class, which is a type of non-linearity.
And in the conclusion, the results are quite good. And this got adopted.
But in the conclusion, there is this honest statement that we offer no explanation except for, this is divine benevolence.
So there you go. This is the extent of our understanding.
OK, so now let's talk about this bitter lesson that I'm sure people have heard about.
I think there's a misconception that a bitter lesson means that scale is all that matters, algorithms don't matter.
All you do is pump more capital into building the model and you're good to go. I think this couldn't be further from the truth.
I think the right interpretation is that algorithms at scale is what matters. And because at the end of the day, your accuracy of your model
is really a product of your efficiency and the number of resources you put in.
And actually, efficiency, if you think about it, is way more important at larger scale.
Because if you're spending hundreds of millions of dollars, you cannot afford to be wasteful in the same way that
if you're looking at running a job on your local cluster,
you might run it again. You fail, you debug it. And if you look at actually the utilization and the use,
I'm sure OpenAI is way more efficient than any of us right now. So efficiency really is important.
And furthermore, I think this point is maybe not as well appreciated in the scaling rhetoric, so to speak.
Which is that if you look at efficiency, which is a combination of hardware and algorithms, but if you just look at the algorithm efficiency,
there's this nice open ed paper from 2020 that showed,
over the period of 2012 to 2019, there was a 44x algorithmic
efficiency improvement in the time that it took to train ImageNet to a certain level of accuracy.
Right, so this is huge. And I think if you-- I don't know if you could see the abstract here.
This is faster than Moore's law. Right, so algorithms do matter. If you didn't have this efficiency,
you would be paying 44 times more cost. This is for image models, but there's some results
for language as well. OK, so with all that, I think the right framing or mindset
to have is, what is the best model one can build given a certain compute and data budget?
OK. And this question makes sense no matter what scale you're at, because it's accuracy per resources.
And of course, if you can raise the capital and get more resources, you'll get better models. But as researchers, our goal is to improve the efficiency
of the algorithms. OK, so maximize efficiency. We're going to hear a lot of that.
OK, so now let me talk a little bit about the current landscape.
And a little bit of, I guess, obligatory history. So language models have been around for a while now.
Going back to Shannon, who looked at language models as a way to estimate the entropy of English.
I think in AI, they really were prominent in NLP, where they were a component of larger
systems like machine translation and speech recognition. And one thing that's maybe not as appreciated these days
is that if you look back in 2007, Google was training fairly large N-gram models.
So 5-gram models, over two trillion tokens, which is a lot more tokens than GPT-3.
And it was only, I guess, in the last two years that we've gotten to that in token count.
But they were N-gram models, so they didn't really exhibit any of the interesting phenomena that we know of language models today.
OK, so in the 2010s, I think a lot of the-- you can think about this. A lot of the deep learning revolution
happened and a lot of the ingredients falling into place. Right, so there was the first neural language
model from Joshua Bengio's group and back in 2003. There was seq-to-seq models.
This, I think, was a big deal for, how do you basically model sequences?
From Ilya and Google folks. There's an Adam optimizer, which still
is used by the majority of people dating over a decade ago. There's a tension mechanism, which
was developed in the context of machine translation, which then
led up to the famous attention all you need, or AKA, the transformer paper in 2017.
People were looking at how to scale mixture of experts. There was a lot of work around late 2010s on how to essentially
do model parallelism. And they were actually figuring out how you could train 100 billion parameter models.
They didn't train it for very long, because these were more systems work. But all the ingredients were in place before by the time
the 2020 came around. So I think one other trend, which was starting NLP,
was the idea of these foundation models that could be trained on a lot of text and adapted to a wide range of downstream tasks.
So ELMo, BERT, T5. These were models that were, for their time, very exciting.
We maybe forget how excited people were about things like BERT. But it was a big deal.
I mean, this is abbreviated history, but I think one critical piece of the puzzle is OpenAI.
Taking these ingredients and applying very nice engineering
and really pushing on the scaling laws. Embracing it as this is the mindset piece.
And that led to GPT-2 and GPT-3. Google obviously was in the game and trying to compete as well.
But that paved the way, I think, to another line
of work, which is these were all closed models. So models that weren't released and you can only access via API.
But there were also open models, starting with the early work by Eleuther right after GPT-3 came out.
Meta's early attempt, which didn't work maybe as quite as well.
BLOOM. And then Meta, Alibaba, DeepSeek, AI2, and there's
a few others which I haven't listed, have been creating these open models where
the weights are released. One other piece of, I think, tidbit about openness,
I think is important, is that there's many levels of openness. There's closed models, like GPT-4.
There's open weight models, where the weights are available. And there's actually a paper, a very nice paper
with lots of architectural details, but no details about the data set. And then there's open source models, where all the weights
and data are available. And the paper where they're honestly trying to explain as much as they can.
But of course, you can't really capture everything in a paper, and there's no substitute for learning how to build it
except for doing it yourself. OK, so that leads to the present day,
where there's a whole host of frontier models from OpenAI,
Anthropic, xAI, Google, Meta, DeepSeek, Alibaba, Tencent, and probably a few others that dominate the current landscape.
So we're in an interesting time, where just to reflect,
a lot of the ingredients, like I said, were developed. Which is good, because I think we're going to revisit some of those ingredients
and trace how these techniques work. And then we're going to try to move as close as we can
to best practices on frontier models, but using information from essentially the open community.
And reading between the lines from what we know about the closed models.
OK, so just as an interlude. So what are you looking at here?
So this is an executable lecture. So it's a program where I'm stepping through,
and it delivers the content of lecture. So one thing that I think is interesting here is that you can embed code.
So you can just step through code. And I think this is a smaller screen than I'm used to,
but you can look at the environment variables as you're stepping through code. So that's useful later when we start
actually trying to drill down and giving code examples. You can see the hierarchical structure of a lecture
like we're in this module and you can see where it was called from main. And you can jump to definitions, like supervised fine tuning,
which we'll talk about later. OK. And if you think this looks like a Python program,
well, it is a Python program. But I've processed it so for your viewing pleasure.
OK. So let's move on to the course logistics now.
Actually, maybe I'll pause for questions.
Any questions about what we're learning in this class?
Yeah. So would you expect graduates in this class
to be able to lead a team to build a frontier model or other skills [INAUDIBLE]?
So the question is, would I expect a graduate from this class to be able to lead a team and build a frontier model?
Of course with $1 billion of capital. Yeah, of course. I would say that it's a good step.
But there's definitely many pieces that are missing. And I think we thought about we should really
teach a series of classes that eventually leads up to as close
as we can get. But I think this is maybe the first step of the puzzle. But there are a lot of things, and I'm
happy to talk offline about that. But I like the ambition. Yeah. That's what you should be doing, taking the class
so you can go lead teams and build frontier models. OK.
OK, let's talk a little bit about the course. So here's a website. Everything's online.
This is a five unit class. But I think that maybe it doesn't express the level
here as well as this quote that I pulled out from a course evaluation. The entire assignment was approximately the same amount
of work as all five assignments from the CS 224n plus the final project. And that's the first homework assignment.
So not to scare you off, but just giving some data here.
So why should you endure that? Why should you do it? I think this class is really for people
who have this obsessive need to understand how things work, all the way down to the atom, so to speak.
And I think when you get through this class, I think you will have really leveled up
in terms of your research, engineering, and the level of comfort that you'll have in building ML systems at scale will just be, I think,
something. There's also a bunch of reasons that you shouldn't take the class.
For example, if you want to get any research done this quarter, maybe this class isn't for you.
If you're interested in learning just about the hottest new techniques, there are many other classes
that can probably deliver on that better than, for example, you spending
a lot of time debugging BPE. And this is really, I think, a class
about the primitives and learning things bottom up as opposed to the latest.
And also if you're interested in building language models or forex, this is probably not the first class you would take.
I think, practically speaking, as much as I made fun of prompting, prompting is great.
Fine tuning is great. If you can do that and it works, then I think that is something you should absolutely start with.
So I don't want people taking this class and thinking that, great, any problem, the first step
is to train a language model from scratch. That is not the right way of thinking about it.
OK. And I know that many of you-- some of you are enrolled, but we did
have a cap, so we weren't able to enroll everyone. And although for the people online, you can follow at home,
all the lecture materials and assignments are online, so you can look at them. The lectures are also recorded and will
be put on YouTube, although there will be some number of week lag there.
And also we'll offer this class next year. So if you were not able to take it this year, don't fret,
there will be next time. OK, so the class has five assignments.
And each of the assignments, we don't provide scaffolding code.
In the sense that literally giving you a blank file and you're supposed to build things up.
And in the spirit of building from scratch. But we're not that mean.
We do provide unit tests and some adapter interfaces that allow you to check correctness of different pieces.
And also the assignment write-up, if you walk through it, does do it for a gentle job of doing that.
But you are on your own for making good software design decisions and figuring out what you name your functions
and how to organize your code. Which is a useful skill, I think.
So one strategy I think, for all assignments is that there is a piece of assignment, which is just implement the thing and make sure
it's correct. That mostly you can do locally on your laptop. You shouldn't need compute for that.
And then we have a cluster that you can run for benchmarking both accuracy and speed.
Right, so I want everyone to embrace this idea of that you want to use a small data set
or as few resources as possible to prototype before running large jobs. You shouldn't be debugging with one billion parameter models
on the cluster if you can help it. OK.
There's some assignments which will have a leaderboard, which
usually is of the form, do things to make perplexity go down given a particular training budget.
Last year it was, I think, pretty exciting for people to try different things that you either
learn from the class or you read online. And then finally, I guess this year is--
this was less of a problem last year, because I guess Copilot wasn't as good. But Cursor's pretty good.
So I think our general strategy is that AI tools can take away
from learning, because there are cases where it can just solve the thing you want it to do.
But I think you can obviously use them judiciously. But use it at your own risk.
You're responsible for your own learning experience here. OK, so we do have a cluster.
So thank you, Together AI, for providing a bunch of H100s for us. There's a guide to please read it carefully to learn
how to use the cluster. And start your assignments early, because the cluster will fill up towards the end of a deadline
as everyone's trying to get their large runs in.
OK. Any questions about that? You mentioned it was a five unit process before.
We were able to sign up for [INAUDIBLE]. Right. So the question is, can you sign up for less than five units.
I think administratively, if you have to sign up for less, that is possible.
But it's the same class and the same workload. Yeah.
Any other questions?
OK. So in this part, I'm going to go through all
the different components of the course and just give a broad overview, a preview of what you're going to experience.
So remember, it's all about efficiency. Given hardware and data, how do you
train the best model given your resources? So for example, if I give you a Common Crawl dump, a web dump,
and 32 H100s for two weeks, what should you do? There are a lot of different design decisions.
There's questions about the tokenizer, the architecture, systems optimizations you can do, data things you can do.
And we've organized the class into these five units or pillars.
So I'm going to go through each of them in turn and talk about what we'll cover, what
the assignment will involve. And then I'll wrap up.
OK, so the goal of the basics unit is just get a basic version of a full pipeline working.
So here, you implement a tokenizer, model architecture, and training. So I'll just say a bit more about what these components are.
So a tokenizer is something that converts between strings
and sequences of integers. Intuitively, you can think about the integers corresponding
to breaking up the string into segments and mapping each segment to an integer.
And the idea is that your sequence of integers is what goes into the actual model, which
has to be a fixed dimension. OK, so in this course, we'll talk
about the byte pair encoding BPE tokenizer, which is relatively
simple and still is used.
There are a promising set of methods
on tokenizer-free approaches. So these are methods that just start with the raw bytes
and don't do tokenization and develop a particular architecture that just takes the raw bytes.
This work is promising. But so far, I haven't seen it been scaled to the frontier yet.
So we'll go with BP for now. OK. So once you've tokenized your sequence or strings
into a sequence of integers, now we define a model architecture over these sequences.
So the starting point here is original transformer. That's what is the backbone of basically all frontier models.
And here's our architectural diagram. We won't go into details here, but there's a tension piece
and then there's a MLP layer with some normalization.
So a lot has actually happened since 2017, right.
And I think there's a sense to which all the transformer was invented and then everyone's just using transformer.
And to a first approximation, that's true. We're still using the same recipe. But there have been a bunch of smaller improvements that
do make a substantial difference when you add them all up. So for example, there is the non-linear activation function.
So SwiGLU, which we saw a little bit before. Positional embeddings. There's new positional embeddings.
These rotary positional embeddings, which we'll talk about. Normalization.
Instead of using layer norm, we're going to look at something called RMS norm, which is similar but simpler.
There's a question of where you place the normalization which has been changed from the original transformer.
The MLP use the canonical version is a dense MLP, and you can replace that with mixture of experts.
Attention is something that has actually been gaining a lot of attention, I guess.
There's full attention, and then there's sliding window attention and linear attention.
All of these are trying to prevent the quadratic blowup. There's also lower dimensional versions, like GQA and MLA,
which we'll get to in a second. Or not in a second, but in a future lecture.
And then the most maybe radical thing is other alternatives to the transformer,
like state space models like Hyena, where they're not doing attention,
but some other operation. And sometimes you get best of both worlds
by mixing, making a hybrid model that mixes these in with transformers.
OK, so once you define your architecture, you need to train. So design decisions include optimizer.
So AdamW, which is a variant basically Adam fixed up,
is still very prominent. So we'll mostly work with that. But it is worth mentioning that there
is more recent optimizers like Muon and SOAP that have shown promise.
Learning rate schedule. Batch size. Whether you do regularization or not.
Hyperparameters. There's a lot of details here. And I think this class is one where the details do matter,
because you can easily have order of magnitude difference between a well tuned architecture
and something that's just a vanilla transformer. So in Assignment 1, basically you'll
implement the BPE tokenizer. I'll warn you that this is actually
the part that seems to have been a lot of surprising, maybe a lot of work for people.
So just, you're warned. And you also implement the transformer cross-entropy loss,
the AdamW optimizer and training loop. So again, the whole stack. And we're not making you implement PyTorch from scratch.
So you can use PyTorch. But you can't use the transformer implementation
for PyTorch. There's a small list of functions that you can use,
and you can only use those. OK, so we're going to have some tiny stories
and open web text data sets that you'll train on. And then there will be a leaderboard to minimize the OpenWeb text perplexity.
We'll give you 90 minutes on an H100 and see what you can do. So this is last year.
So see, we have the top. So this is the number to beat for this year. OK.
All right. So that's the basics. Now after basics, I mean, in some sense, you're done, right.
You have ability to train a transformer. What else do you need? So the system part really goes into how
you can optimize this further. So how do you get the most out of hardware? And for this, we need to take a closer look at the hardware
and how we can leverage it. So there's kernels, parallelism, and inference
are the three components of this unit. So OK, so to first talk about kernels.
Let's talk a little bit about what a GPU looks like. OK, so a GPU, which we'll get much more into,
is basically a huge array of these little units that
do floating point operations. And maybe the one thing to note is that this is the GPU chip,
and here is the memory that's actually off chip.
And then there's some other memory, like L2 caches and L1 caches on chip.
And so the basic idea is that compute has to happen here.
Your data might be somewhere else. And how do you basically organize your compute so that you can be most efficient?
So one quick analogy is imagine that your memory
is where you can store your data and the model parameters is
like a warehouse. And your compute is like the factory. And what ends up being a big bottleneck
is just data movement costs, all right. So the thing that we have to do is,
how do you organize the compute, even a matrix multiplication, to maximize the utilization of the GPUs
by minimizing the data movement? And there's a bunch of techniques like fusion and tiling that allow you to do that.
So we'll get all into the details of that. And to implement and leverage the kernel,
we're going to look at Triton. There's other things you can do with various levels of sophistication, but we're going
to use Triton, which is developed by OpenAI in a popular way to build kernels. OK, so we're going to write some kernels.
That's for one GPU. So now, in general, you have these big runs take 10,000s,
if not tens of thousands of GPUs. But even at eight, it starts becoming interesting,
because you have a lot of GPUs that are connected to some CPU nodes.
And they also are directly connected via NVSwitch, NVLink.
And it's the same idea, right.
The only thing is that data movement between GPUs is even slower. Right, and so we need to figure out
how to put model parameters and activations and gradients
and put them on the GPUs and do the computation and to minimize the amount of movement.
And then so we're going to explore different type of techniques, like data parallelism and tensor
parallelism, and so on. So that's all I'll say about that.
And finally, inference is something that we didn't actually do last year in the class.
Although we had a guest lecture. But this is important, because inference is how you actually
use a model. Right, it's basically the task of generating tokens given a prompt, given a trained model.
And it also turns out to be really useful for a bunch of other things besides just chatting with your favorite model.
You need it for reinforcement learning, test time compute, which has been very popular lately.
And even evaluating models, you need to do inference. So we're going to spend some time talking about inference.
Actually, if you think about globally, the cost that's spent on inference,
it's eclipsing the cost that it's used to train models.
Because training, despite it being very intensive, is ultimately a one-time cost. And inference is-- cost scales with every use.
And the more people use your model, the more you'll need inference to be efficient.
OK, so in inference, there's two phases.
There's a pre-fill and a decode. Pre-fill is you take the prompt and you can run it through the model and get some activations.
And then decode is you go autoregressively one by one and generate tokens.
So pre-fill, all the tokens are given, so you can process everything at once.
So this is exactly what you see at training time. And generally, this is a good setting
to be in, because you can parallel-- it's naturally parallel and you're mostly compute bound.
What makes inference, I think, it's special and difficult is that this autoregressive decoding,
you need to generate one token at a time. And it's hard to actually saturate all your GPUs, and it becomes memory bound because you're constantly
moving data around. And we'll talk about a few ways to speed the models up,
to speed inference up. You can use a cheaper model. You can use this really cool technique
called speculative decoding, where you use a cheaper model to scout ahead and generate
multiple tokens. And then if these tokens happen to be good for some definition
good, you can have the full model just score and accept them all in parallel.
And then there's a bunch of systems optimizations that you can do as well. OK, so after the systems--
oh, OK, Assignment 2. So you're going to implement a kernel.
You're going to implement some parallelism. So data parallel is very natural, and so we'll do that.
Some of the model parallelism, like FSDP, turns out to be a bit complicated to do from scratch.
So we'll do a baby version of that. But I encourage you to learn and know about the full version.
We'll go over the full version in class, but implementing from scratch might be a bit too much.
And then I think an important thing is getting in the habit of always benchmarking and profile. I think that's actually probably the most important thing, is
that you can implement things. But unless you have feedback on how well your implementation is
going and where the bottlenecks are, you're just going to be flying blind.
OK, so Unit 3 is scaling laws.
And here, the goal is you want to do experiments at small scale and figure things out, and then predict the hyperparameters
and loss at large scale. So here's a fundamental question.
So if I give you a FLOPs budget, what model size should you use?
If you use a larger model, that means you can train on less data. And if you use a smaller model, you can train on more data.
So what's the right balance here? And this has been studied quite extensively
and figured out by a series of papers from OpenAI and DeepMind. So if you hear the term Chinchilla optimal,
this is what this is referring to. And the basic idea is that for every compute budget
number of FLOPSs, you can vary the number of parameters of your model.
OK, and then you measure how good that model is. So for every level of compute, you
can get the optimal parameter count. And then what you do is you can fit a curve to extrapolate
and see if you had, let's say, 1e22 FLOPs, what would there be the parameter size?
And it turns out these minima, when you plot them, it's actually remarkably linear.
Which leads to this very actually simple but useful rule
of thumb, which is that if you have a particular model of size
n, if you multiply by 20, that's the number of tokens you should train on, essentially.
So that means if I say 1.4 billion parameter model should be trained on 28 billion tokens.
OK. But this doesn't take into account inference cost. This is literally, how can you train the best model, regardless
of how big that model is? So there's some limitations here, but it's nonetheless been extremely useful
for model development. So in this assignment, this is fun, because we define a, quote,
unquote, "training API," which you can query with a particular set of hyperparameters.
You specify the architecture and batch size and so on. And we return you a loss that your decisions will get you.
OK, so your job is you have a FLOPs budget. And you're going to try to figure out
how to train a bunch of models and then gather the data. You're going to fit a scaling law to the gathered data.
And then you're going to submit your prediction on what you would choose to be the hyperparameters.
What model size and so on at a larger scale.
OK. So this is a case where you have to be really-- we want to put you in this position
where there's some stakes. I mean, this is not like burning real compute. But once you run out of your FLOPs budget, that's it.
So you have to be very careful in terms of how you prioritize what experiments to run.
Which is something that the frontier labs have to do all the time. And there will be a leaderboard for this,
which is, minimize loss given the FLOPs budget.
Question? So those are things from 2024.
Yeah, so if we're working ahead, should we expect assignments to change over time,
or are these going to be the final-- Yeah, so the question is that these links are from 2024.
The rough assignments, the rough structure will be the same for 2025. There will be some modifications.
But if you look at these, you should have a pretty good idea of what to expect.
OK, so let's go into data now. OK, so up until now, you have scaling laws, you have systems.
You have your transformer implementation and everything. You're really good to go.
But data, I would say, is a really key ingredient that I think differentiates in some sense.
And the question to ask here is, what do I want this model to do? Right, because what the model does is completely determine--
I mean, mostly determined by the data. If I train a multilingual data, it
will have multilingual capabilities. If I trained on code, it will have code capabilities. And it's very natural.
And usually data sets are a conglomeration of a lot of different pieces.
This is from a pile, which is four years ago. But the same idea, I think, holds.
You have data from the web. This is Common Crawl. You have StackExchange, Wikipedia, GitHub,
and different sources, which are curated. And so in the data section, we're
going to start talking about evaluation, which is given a model, how do you evaluate whether it's any good?
So we're going to talk about perplexity measures,
standardized testing, like MMLU. If you have models that generate utterances
for instruction following, how do you evaluate that? There's also decisions about, if you can ensemble or do
train of thought at test time, how does that affect your evaluation? And then you can talk about entire systems, evaluation
of entire system, not just the language model, because language models often get these days plugged into some agentic system or something.
OK, so now after establishing evaluation, let's look at data curation.
So this is, I think, an important point that people don't realize. I often hear people say, oh, we're training
the model on the internet. This just doesn't make sense. Right, data doesn't just fall from the sky
and there's the internet that you can pipe into your model.
Data has to always be actively acquired somehow.
So even just as an example of--
I always tell people, look at the data. And so let's look at some data.
So this is some Common Crawl data.
I'm going to take 10 documents. And I think hopefully, this works. OK.
I think the rendering is off, but you can see this is a random sample of Common Crawl.
And you can see that this is maybe not exactly the data.
Oh, here's some actually real text here. OK, that's cool. But if you look at most of Common Crawl, aside--
this is a different language. But you can also see this is very spammy sites. And you'll quickly realize that a lot of the web is just trash.
And so-- well, OK, maybe that's not that's surprising, but it's more trash than you would actually expect,
I promise. So what I'm saying is that there's a lot of work
that needs to happen in data. So you can crawl the internet. You can take books, archives of papers, GitHub.
And there's actually a lot of processing that needs to happen. There's also legal questions about what data you can
train on, which we'll touch on. Nowadays, a lot of frontier models have to actually buy data.
Because the data on the internet that's publicly accessible actually turns out to be a bit limited for the really frontier
performance. And also, I think it's important to remember that this data that's scraped, it's not actually text, right.
First of all, it's HTML or it's PDFs. Or in the case of code, it's just directories.
So there has to be an explicit process that takes this data and turns it into text.
OK, so we're going to talk about the transformation from HTML to text.
And this is going to be a lossy process. So the trick is, how can you preserve the content and some
of the structure without basically just having an HTML?
Filtering, as you could surmise, is going to be very important, both for getting high quality data
but also removing harmful content. Generally, people train classifiers to do this.
Deduplication is also an important step, which we'll talk about.
OK, so Assignment 4 is all about data. We're going to give you the raw Common Crawl dump so you
can see just how bad it is. And you're going to train classifiers, dedupe.
And then there's going to be a leaderboard where you're going to try to minimize perplexity given
your token budget. So now you have the data.
You've done this-- build all your fancy kernels. Now you can really train models.
But at this point, what you'll get is a model that can complete the next token.
Right. And this is called essentially a base model. And I think about it as a model that has a lot of raw potential.
But it needs to be aligned or modified in some way. And alignment is a process of making it useful.
So alignment captures a lot of different things.
But three things I think it captures is that you want to get the language model to follow instructions.
Right, complete the next token is not necessarily following the instruction. It will just complete the instruction
or whatever it thinks will follow the instruction. You get to here, specify the style of the generation.
Whether you want it to be long or short, whether you want bullets. Whether you want it to be witty or have SaaS or not.
And when you play with ChatGPT versus Grok, you'll see that there's different alignment that
has happened. And then also safety. One important thing is for these models
to be able to refuse answers that can be harmful. So that's where alignment also kicks in.
So there's generally two phases of alignment. There's supervised fine tuning.
And here, the goal is-- I mean, it's very simple. You basically gather a set of user assistant pairs.
So prompt response pairs. And then you do supervised learning.
OK. And the idea here is that the base model already has the raw potential.
So just fine tuning it on a few examples is sufficient.
Of course, the more examples you have, the better the results. But there's papers like this one that
shows even a thousand examples suffices to give you instruction following capabilities
from the good base model. OK, so this part is actually very simple.
And it's not that different from pre-training, because it's just you're given text and you just maximize the probability of the text.
So the second part is a bit more interesting from an algorithmic perspective. So the idea here is that even with an SFT phase,
you will have a decent model. And now, how do you improve it?
Well, you can gather more SFT data, but that can be very expensive because you have to have someone sit down and annotate data.
So the goal of learning from feedback is that you can leverage lighter forms of annotation
and have the algorithms do a bit more work. OK, so one type of data you can learn from is preference data.
So this is where you generate multiple responses from a model to a given prompt, like A or B.
And the user rates whether A or B is better. And so the data might look like it generates--
what's the best way to train a language model? Use a large data set or use a small data set. And of course, the answer should be A. So
that is a unit of expressing preferences. Another type of supervision you could have is using verifiers.
So for some domains, you're lucky enough to have a formal verifier, like for math or code.
Or you can use learned verifiers where you train an actual language model to rate the response.
And of course, this relates to evaluation again. Algorithms.
We're in the realm of reinforcement learning. So one of the earliest algorithms
that was developed that was applied to instruction tuning models was PPO, Proximal Policy Optimization.
It turns out that if you just have preference data, there's a much simpler algorithm called DPO that works really well.
But in general, if you wanted to learn from verifier's data, you have to-- it's not preference data,
so you have to embrace RL fully. And there's this method, which we'll
do in this class, which is called group relative preference optimization, which simplifies PPO and makes it more efficient
by removing the value function developed by DeepSeek, which seems to work pretty well.
OK, so Assignment 5 implements supervised tuning, DPO and GRPO.
And of course, evaluate. Question.
I think a quote from the course [INAUDIBLE] about Assignment 1-- did people have similar things to say about Assignments 2
through 5? Yeah, the question is Assignment 1 seems a bit daunting.
What about the other ones? I would say that Assignment 1 and 2 are definitely the most heavy and hardest.
Assignment 3 is a bit more of a breather. And Assignment 4 and 5, at least last year,
were, I would say, a notch below Assignment 1 or 2. Although, I don't know, it depends on--
we haven't fully worked out the details for this year.
Yeah, it does get better. OK, so just to recap of the different pieces here.
Remember, efficiency is this driving principle. And there's a bunch of different design decisions.
I think if you view efficiency, everything through the lens of efficiency, I think a lot of things
make sense. And importantly, I think we are--
it's worth pointing out there, we are currently in this compute constrained regime,
at least this class and most people who are somewhat GPU poor. So we have a lot of data, but we don't have that much compute.
And so these design decisions will reflect squeezing the most out of the hardware. So for example, data processing.
We're filtering fairly aggressively because we don't want to waste precious compute on bad or irrelevant data.
Tokenization. It's nice to have a model over bytes.
That's very elegant. But it's very compute inefficient with today's model architectures. So we have to do tokenization as an efficiency gain.
Model architecture, there are a lot of design decisions there that are essentially motivated by efficiency training.
I think the fact that most of what we're going to do is just a single epoch. This is clearly we're in a hurry.
We just need to see more data as opposed to spend a lot of time on any given data point.
Scaling laws is completely about efficiency. We use less compute to figure out the hyperparameters.
And alignment is maybe a little bit different.
But the connection to efficiency is that if you can put resources
into alignment, then you actually require smaller base models.
OK. So there's two paths. If your use case is fairly narrow,
you can probably use a smaller model. You align it or fine tune it, and you can do well. But if your use cases are very broad,
then there might not be a substitute for training a big model. So that's today.
So increasingly now, at least for Frontier Labs, they're becoming data constrained.
Which is interesting because I think that the design decisions will presumably completely change.
Well, I mean, compute will always be important, but I think the design decisions will change. For example, taking one epoch of your data, I think,
doesn't really make sense. If you have more compute, why wouldn't you take more epochs at least, or do something smarter?
Or maybe there will be different architectures, for example. Because the transformer was really motivated by
compute efficiency. So that's something to ponder. Still, it's about efficiency.
But the design decisions reflect what regime you're in.
OK. So now I'm going to dive into the first unit.
Before that, any questions?
Do you have a Slack or Ed? The question is, do we have a Slack or Ed? We will have a Slack.
We'll send out details after this class. Yeah. Will students auditing the course also have access
to the same material? The question is, students auditing the class will have access to all the online materials,
assignments. And we'll give you access to Canvas so you can watch the lecture videos.
Yeah. What's the grading of the assignments? What's the grading of the assignments?
Good question. So there will be a set of unit tests that you will have to pass.
So part of the grading is just, did you implement this correctly? There will be also parts of the grade which will--
did you implement a model that achieved a certain level of loss or is efficient enough?
In the assignment, every problem part has a number of points associated with it.
And so that gives you a fairly granular level of what grading looks like.
OK, let's jump into tokenization.
OK, so Andrej Karpathy has this really nice video on tokenization. And in general, he makes a lot of these videos on that actually
inspired a lot of this class. How you can build things from scratch. So you should go check out some of his videos.
So tokenization, as we talked about it, is the process of taking raw text, which is generally
represented as Unicode strings, and turning it into a set of integers essentially.
And where each integer represents a token. OK, so we need a procedure that encodes strings to tokens
and decodes them back into strings. And the vocabulary size is just the number of values
that a token take on the number of the range of the integers. So just to give you an example of how tokenizers work,
let's play around with this really nice website, which allows you to look at different tokenizers and just
type in something like hello. Hello, or whatever.
Maybe I'll do this. And one thing it does is it shows you the list of integers.
This is the output of tokenizer. It also nicely maps out the decomposition
of the original string into a bunch of segments. And a few things to note.
First of all, the space is part of a token. So unlike classical NLP where the space just disappears,
everything is accounted for. These are meant to be reversible operations, tokenization.
And by convention, for whatever reason, the space is usually preceding the token.
Also notice that hello is a completely different token than space hello, which might make you a little bit squeamish.
And it can cause problems, but that's just how it is. Question? Can I ask, is the space being leading instead of trailing
intentional, or is it just an artifact of the BPE process? So the question is, is the spacing
before intentional or not? So in the BPE process that we'll talk about,
you're actually pre-tokenized, and then you tokenize each part.
And I think the pre-tokenizer does put the space in the front. So it is built into the algorithm.
You could put it at the end. But I think it probably makes more sense to put it in the beginning.
But I actually don't-- well, I guess it could go either way.
If it makes sense. OK, so then if you look at numbers,
you see that the numbers are chopped down into different pieces.
It's a little bit interesting that it's left to right. So it's definitely not grouping by thousands or anything
semantic. But anyway, I encourage you to play with it and get a sense of what these existing tokenizers look like.
So this is the tokenizer for GPT 4.0, for example.
So there are some observations that we made. So if you look at the GPT-2 tokenizer,
which will use this as a reference-- OK, let me see if I can--
hopefully this is-- let me know if this is getting too small in the back.
You can take a string. If you apply the GPT-2 tokenizer, you get your indices.
So it maps strings to indices. And then you can decode to get back the string.
And this is just a sanity check to make sure that you actually round trips.
Another thing that I guess interesting to look at is this compression ratio, which is if you look at the number of bytes divided
by the number of tokens. So how many bytes are represented by a token? And the answer here is 1.6.
OK, so every token represents 1.6 bytes of data.
OK, so that's just a GPT tokenizer that OpenAI trained. To motivate BPE, I want to go through a sequence of attempts.
So suppose you wanted to do tokenization. What would be the simplest thing?
The simplest thing is probably character-based tokenization. A Unicode string is a sequence of Unicode characters.
And each character can be converted into an integer and called a code point.
OK, so A maps to 97. The world emoji maps to 127,757.
And you can see that it converts back. OK, so you can define a tokenizer, which simply maps each character into a code point.
OK. So what's one problem with this?
Yeah? Compression ratio is one. The compression ratio is one. So that's-- well, actually, the compression ratio is not quite
one because a character is not a byte. But it's maybe not as good as you want.
One problem with that, if you look at some code points, they're actually really large, right.
So you're basically allocating each one slot in your vocabulary
for every character uniformly. And some characters appear way more frequently than others.
So this is not a very effective use of your budget.
OK. So the vocabulary size is huge. I mean, the vocabulary size being 127
is actually a big deal. But the bigger problem is that some characters are rare,
and this is inefficient use of the vocab. OK, so the compression ratio is 1.5 in this case,
because it's the tokens-- sorry, the number of bytes per token. And a character can be multiple bytes.
OK, so that was a very naive approach. On the other hand, you can do byte-based tokenization.
OK, so Unicode strings can be represented as a sequence of bytes.
Because every string can just be converted into bytes OK,
so A is already just one byte. But some characters take up as many as four bytes.
And this is using the UTF-8 encoding of Unicode. There's other encodings, but this is the most common one
that's dynamic. So let's just convert everything into bytes and see what happens.
So if you do it into bytes, now all the indices are between 0 and 256, because there are only
256 possible values for a byte by definition. So your vocabulary is very small.
And each byte is-- I guess not all bytes are equally used, but it's not--
you don't have that many sparsity problems. But what's the problem with byte-based encoding?
Long sequences. Yeah, long sequences.
I mean, in some ways, I really wish byte coding would work. It's the most elegant thing.
But you have long sequences. Your compression ratio is 1, one byte per token.
And this is just terrible. A compression ratio of 1 is terrible, because your sequences will be really long.
Attention is quadratic naively in the sequence length. So you're just going to have a bad time in terms of efficiency.
OK. So that wasn't really good. So now the thing that you might think about
is, well, maybe we have to be adaptive here. Right, we can't allocate a character or a byte per token.
But maybe some tokens can represent lots of bytes and some tokens can represent few bytes. So one way to do this is word-based tokenization.
And this is something that was actually very classic in NLP. OK, so here's a string.
And you can just split it into a sequence of segments.
OK. And you can call each of these tokens. So you just use a regular expression.
Here's a different regular expression that GPT-2 uses to pre-tokenize.
And it just splits your string into a sequence of strings.
So and then what you do with each segment is you assign each of these to an integer,
and then you're done. OK. So what's the problem with this?
Vocabulary size. Yeah, so the problem is that your vocabulary size
is unbounded. Or not. Maybe not quite unbounded, but you don't know how big it is.
Right, because on a given new input, you might get a segment that just you've never seen before.
And that's actually a big problem. Word base is a really big pain in the butt,
because some real words are rare. And actually, it's really annoying
because new words have to receive this UNC token. And if you're not careful about how you compute the perplexity,
then you just mess up. So word based isn't--
I think it captures the right intuition of adaptivity. But it's not exactly what we want here.
So here, we're finally going to talk about the BPE encoding or Byte Pair Encoding.
So this was actually a very old algorithm developed by Phillip Gage in '94 for data compression.
And it was first introduced into NLP for neural machine translation.
So before, papers that did machine translation or any-- basically, all NLP used word based tokenization.
And again, word base with a pain. So this paper pioneered this idea, well, we
can use this nice algorithm form 94, and we can just make the tokenization round trip.
And we don't have to deal with UNCs or any of that stuff. And then finally, this entered the language modeling era
through GPT-2, which was trained on using the BPE tokenizer.
OK, so the basic idea is instead of defining some preconceived notion of how to split it up,
we're going to train the tokenizer on raw text. That's the basic insight, if you will.
And so organically, common sequences that span multiple characters, we're
going to try to represent as one token. And rare sequences are going to be represented by multiple tokens.
There's a slight detail, which is for efficiency, the GPT-2 paper uses word based tokenizer as a pre-processing
to break it up into segments, and then runs BP on each of the segments, which is what you're going to do in this class as well.
The algorithm BP is actually very simple. So we first convert the string into a sequence
of bytes, which we already did when we talked about byte-based tokenization. And now we're going to successively
merge the most common pair of adjacent tokens over and over again. So the intuition is that if a pair of tokens shows up a lot,
then we're going to compress it into one token. We're just going to dedicate space for that.
OK, so let's walk through what this algorithm looks like. So we're going to use this cat and hat as an example.
And we're going to convert this into a sequence of integers.
These are the bytes. And then we're going to keep track of what we've merged.
So remember, merges is a map from two integers which
can represent bytes or other pre-existing tokens. And we're going to create a new token.
And the vocab is just going to be a handy way to represent the index to bytes.
The BPE algorithm, I mean, it's very simple,
so I'm just actually going to run through the code. You're going to do this num merges of times. So num merges is 3, in this case.
We're going to first count up the number of occurrences of pairs of bytes.
So hopefully, this doesn't become too small. So we're going to just step through this sequence,
and we're going to see that-- OK, so what's 116, 104? We're going to increment that count.
104, 101. Increment that count. We're going to go through the sequence, and we're going to count up the bytes.
OK. So now after we have these counts, we're going to find the pair that occurs
the most number of times. So I guess there's multiple ones, but we're just going to break ties and say 116 and 104.
OK, so that occurs twice. So now we're going to merge that pair.
So we're going to create a new slot in our vocab, which is going to be 256.
So so far, it's 0 through 255. And now we're expanding the vocab to 256.
And we're going to say every time we see 116 and 104, we're going to replace it with 256.
OK. And then we're going to just apply
that merge to our training set. So after we do that, the 116 and 104 became 256.
And this 256, remember, occurred twice. OK. So now we're just going to loop through this algorithm one more
time. The second time, it decided to merge 256 and 101.
And now I'm going to replace that in indices. And notice that the indices is going to shrink.
Right, because our compression ratio is getting better as we make room for more vocabulary items
and we have a greater vocabulary to represent everything. OK, so let me do this one more time.
And then the next merge is 257, 3. And this is shrinking one more time.
OK. And then now we're done. OK, so let's try out this tokenizer.
So we have the string, the quick brown fox. We're going to encode into a sequence of indices.
And then we're going to use our BPE tokenizer to decode. Let's actually step through what that looks like.
Well, actually maybe decoding isn't actually interesting.
Sorry, I should have gone through the encode. Let's go back to encode.
So encode, you take a string. You convert it to indices. And you just replay the merges.
Importantly in the order that it occurred. So I'm going to replay these merges.
And then I'm going to get my indices.
OK, and then verify that this works. OK, so that was--
it's pretty simple. Because it's simple, it was also very inefficient.
For example, encode loops over the merges. You should only loop over the merges that matter.
And there's some other bells and whistles. There's special tokens, pre-tokenization.
And so in your assignment, you're going to essentially take this as a starting point.
Or I mean I guess you should implement your own from scratch. But your goal is to make the implementation fast.
And you can parallelize it if you want. You can go have fun. OK, so a summary of tokenization.
So tokenizer maps between strings and sequences of integers.
We looked at character base by base, word base. They're highly suboptimal for various reasons.
BPE is a very old algorithm from '94 that still proves to be an effective heuristic.
And the important thing is that it looks at your corpus statistics to make sensible decisions about how to best adaptively allocate vocabulary to represent
sequences of characters. And I hope that one day, I won't have to give this lecture,
because we'll just have architectures that map from bytes. But until then, we'll have to deal with tokenization.
OK, so that's it for today. Next time, we're going to dive into the details of PyTorch
and give you the building blocks and pay attention to resource accounting. All of you have presumably implemented PyTorch programs,
but we're going to really look at where all the FLOPs are going. OK, see you next time.